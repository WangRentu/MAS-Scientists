"""
LiteratureAgent - MVP æ–‡çŒ®æ£€ç´¢ Agent

å°è£… UnifiedLiteratureSearchï¼Œæä¾›ï¼š
- search_and_summarizeï¼šæŒ‰æŸ¥è¯¢æ£€ç´¢æ–‡çŒ®å¹¶ç”¨ LLM åšç®€è¦æ‘˜è¦
"""

from typing import Any, Dict, List, Optional
import logging

from evoverse.agents.base_agent import BaseAgent
from evoverse.literature.unified_search import UnifiedLiteratureSearch
from evoverse.literature.base_client import PaperMetadata, PaperAnalysis
from evoverse.core.llm_client import LLMClient
import hashlib
import time


logger = logging.getLogger(__name__)


class LiteratureAgent(BaseAgent):
    """MVP ç‰ˆæœ¬æ–‡çŒ® Agentã€‚"""

    def __init__(
        self,
        agent_id: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None,
        llm_client: Optional[LLMClient] = None,
        searcher: Optional[UnifiedLiteratureSearch] = None,
    ):
        super().__init__(agent_id=agent_id, agent_type="LiteratureAgent", config=config)
        self.llm = llm_client or LLMClient(max_history=16)
        self.searcher = searcher or UnifiedLiteratureSearch(
            semantic_scholar_enabled=False
        )

    def search_and_summarize(
        self,
        query: str,
        max_results: int = 20,
    ) -> List[Dict[str, Any]]:
        """
        æ£€ç´¢æ–‡çŒ®å¹¶ä¸ºæ¯ç¯‡ç”Ÿæˆç®€çŸ­æ‘˜è¦ã€‚

        è¿”å›çš„æ¯ä¸ªå…ƒç´ æ˜¯ç»è¿‡ç²¾ç®€çš„ dictï¼Œæ–¹ä¾¿åç»­åºåˆ—åŒ–å’Œ prompt ä½¿ç”¨ï¼š
        {
            "title": ...,
            "authors": [...],
            "year": ...,
            "abstract": ...,
            "summary": ...,
            "source": ...,
            "primary_id": ...
        }
        """
        logger.info("LiteratureAgent searching papers: %s", query)
        papers = self.searcher.search(
            query=query,
            max_results_per_source=max_results,
            total_max_results=max_results,
            deduplicate=True,
            extract_full_text=True,
        )
        for i, p in enumerate(papers, start=1):
            logger.info(
                "Paper %d: [%s] %s",
                i,
                getattr(p, "primary_identifier", getattr(p, "id", "")),
                getattr(p, "title", ""),
            )

        simplified: List[Dict[str, Any]] = []
        for p in papers:
            simplified.append(self._simplify_paper(p))

        # ç”¨ LLM ç”Ÿæˆæ‘˜è¦å’Œæ·±åº¦åˆ†æ
        for item in simplified:
            item["summary"] = self._summarize_paper(item)
            # æ·»åŠ æ·±åº¦åˆ†æé€‰é¡¹ï¼ˆå¯é€‰ï¼‰
            if self.config.get("enable_deep_analysis", False):
                deep_analysis = self.analyze_paper_deep(item)
                item["deep_analysis"] = deep_analysis.to_dict()

        logger.info("LiteratureAgent retrieved %d papers", len(simplified))
        return simplified

    def _simplify_paper(self, paper: Any) -> Dict[str, Any]:
        """å°† PaperMetadata å‹ç¼©æˆæ˜“äºä¼ è¾“å’Œåºåˆ—åŒ–çš„å­—å…¸ï¼Œå¹¶è¡¥å……ç»Ÿä¸€ IDã€‚"""
        # 1. å–åŸºç¡€å­—æ®µ
        title = getattr(paper, "title", "") or ""
        authors = getattr(paper, "authors", []) or []
        year = getattr(paper, "year", None)
        abstract = getattr(paper, "abstract", None) or getattr(paper, "summary", "")
        full_text = getattr(paper, "full_text", None) or ""
        source = getattr(paper, "source", None)
        primary_id = getattr(paper, "primary_identifier", None)

        # 2. è§„èŒƒä½œè€…åˆ—è¡¨ä¸º List[str]
        norm_authors: List[str] = []
        for a in authors:
            if isinstance(a, str):
                norm_authors.append(a)
            else:
                name = getattr(a, "name", None) or str(a)
                norm_authors.append(name)

        # 3. è§„èŒƒ source ä¸ºå­—ç¬¦ä¸²
        if source is None:
            source_str = "unknown"
        else:
            source_str = getattr(source, "value", str(source)).lower()

        # 4. æ„é€ ç»Ÿä¸€é€»è¾‘ IDï¼ˆå’Œ Kosmos ä¸€æ ·çš„æ€è·¯ï¼‰
        global_id = primary_id or f"unknown:{hashlib.sha1(title.encode()).hexdigest()[:16]}"

        # 5. è¿”å›å¸¦ id çš„ç®€åŒ–ç»“æ„
        return {
            "id": global_id,          # ğŸ”´ ç»Ÿä¸€çš„å†…éƒ¨ IDï¼ˆåé¢æ‰€æœ‰åœ°æ–¹éƒ½ç”¨å®ƒï¼‰
            "source": source_str,     # æ–‡çŒ®æ¥æºï¼šarxiv / pubmed / semanticscholar / unknown
            "primary_id": primary_id, # åŸå§‹ source çš„ä¸»é”®ï¼Œæ–¹ä¾¿ debug æˆ–å¤–éƒ¨è·³è½¬
            "title": title,
            "authors": norm_authors,
            "year": year,
            # å¯¹é½ Kosmosï¼šä¿ç•™å…¨æ–‡ï¼Œæ‘˜è¦é˜¶æ®µä¼˜å…ˆä½¿ç”¨
            "full_text": full_text,
            "abstract": abstract,
            "summary": "",            # è¿™é‡Œå…ˆç•™ç©ºï¼Œåé¢ _summarize_paper å†å¡«
        }
    

    def _summarize_paper(self, paper: Dict[str, Any]) -> str:
        """ä½¿ç”¨ LLM ä¸ºå•ç¯‡æ–‡çŒ®ç”Ÿæˆç»“æ„åŒ–åˆ†ææ‘˜è¦ã€‚"""
        title = paper.get("title", "")
        full_text = paper.get("full_text") or ""
        abstract = paper.get("abstract", "")

        # å¯¹é½ Kosmosï¼šä¼˜å…ˆä½¿ç”¨å…¨æ–‡ï¼Œå…¶æ¬¡æ‘˜è¦ï¼Œå¦‚æœéƒ½æ²¡æœ‰å°±ç›´æ¥è¿”å›ç©º
        if full_text:
            base_text = f"å…¨æ–‡ï¼ˆæˆªæ–­ï¼‰ï¼š\n{full_text[:5000]}"
        elif abstract:
            base_text = f"æ‘˜è¦ï¼š\n{abstract}"
        else:
            return ""

        # æ„å»ºä¸“ä¸šåŒ–çš„æ€»ç»“ prompt
        text = f"æ ‡é¢˜ï¼š{title}\n\n{base_text}"

        prompt = f"""åˆ†æè¿™ç¯‡ç§‘å­¦è®ºæ–‡å¹¶æä¾›å…¨é¢çš„æ€»ç»“ã€‚

{text}

è¯·æä¾›ç»“æ„åŒ–åˆ†æï¼ŒåŒ…æ‹¬ï¼š
1. æ‰§è¡Œæ‘˜è¦ï¼ˆ2-3å¥è¯ï¼‰
2. å…³é”®å‘ç°ï¼ˆ3-5ä¸ªä¸»è¦ç»“æœï¼‰
3. ç ”ç©¶æ–¹æ³•ï¼ˆç ”ç©¶æ–¹æ³•æ¦‚è¿°ï¼‰
4. é‡è¦æ€§ï¼ˆç§‘å­¦æ„ä¹‰ï¼‰
5. å±€é™æ€§ï¼ˆå¼±ç‚¹æˆ–æ³¨æ„äº‹é¡¹ï¼‰

è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œç›´æ¥ç»™å‡ºåˆ†æå†…å®¹ã€‚"""

        try:
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯èµ„æ·±çš„ç§‘å­¦æ–‡çŒ®åˆ†æå¸ˆã€‚è¯·æä¾›å…¨é¢ã€å‡†ç¡®çš„åˆ†æã€‚",
                },
                {
                    "role": "user",
                    "content": prompt,
                },
            ]
            summary = self.llm.chat(messages)
        except Exception as exc:  # noqa: BLE001
            logger.warning("LLM summarize paper failed: %s", exc)
            summary = f"è®ºæ–‡ã€Š{title}ã€‹çš„åˆ†ææš‚æ—¶ä¸å¯ç”¨ã€‚"

        return summary

    def analyze_paper_deep(self, paper: Dict[str, Any]) -> PaperAnalysis:
        """
        æ·±åº¦åˆ†æå•ç¯‡è®ºæ–‡ï¼Œè¿”å›ç»“æ„åŒ–çš„ PaperAnalysisã€‚

        Args:
            paper: ç®€åŒ–åçš„è®ºæ–‡æ•°æ®å­—å…¸

        Returns:
            PaperAnalysis: å®Œæ•´çš„è®ºæ–‡åˆ†æç»“æœ
        """
        start_time = time.time()

        # æ„å»ºè®ºæ–‡æ–‡æœ¬
        title = paper.get("title", "")
        full_text = paper.get("full_text", "")
        abstract = paper.get("abstract", "")

        text = f"æ ‡é¢˜ï¼š{title}\n\n"
        if full_text:
            text += f"å…¨æ–‡ï¼ˆæˆªæ–­ï¼‰ï¼š\n{full_text[:5000]}"
        elif abstract:
            text += f"æ‘˜è¦ï¼š\n{abstract}"
        else:
            text += "æ‘˜è¦ï¼šä¸å¯ç”¨"

        # æ„å»ºåˆ†æprompt
        prompt = f"""åˆ†æè¿™ç¯‡ç§‘å­¦è®ºæ–‡å¹¶æä¾›å…¨é¢çš„æ€»ç»“ã€‚

{text}

è¯·æä¾›ç»“æ„åŒ–åˆ†æï¼ŒåŒ…æ‹¬ï¼š
1. æ‰§è¡Œæ‘˜è¦ï¼ˆ2-3å¥è¯ï¼‰
2. å…³é”®å‘ç°ï¼ˆ3-5ä¸ªä¸»è¦ç»“æœï¼Œç”¨åˆ—è¡¨å½¢å¼ï¼‰
3. æ–¹æ³•è®ºï¼ˆç ”ç©¶æ–¹æ³•æ¦‚è¿°ï¼‰
4. é‡è¦æ€§ï¼ˆç§‘å­¦æ„ä¹‰ï¼‰
5. å±€é™æ€§ï¼ˆå¼±ç‚¹æˆ–æ³¨æ„äº‹é¡¹ï¼Œç”¨åˆ—è¡¨å½¢å¼ï¼‰
6. ç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´çš„æ•°å€¼ï¼Œè¡¨ç¤ºåˆ†æçš„ä¿¡å¿ƒæ°´å¹³ï¼‰

è¯·è¿”å›JSONæ ¼å¼çš„å“åº”ã€‚
"""

        # å®šä¹‰è¾“å‡ºæ¨¡å¼
        output_schema = {
            "type": "object",
            "properties": {
                "executive_summary": {"type": "string"},
                "key_findings": {
                    "type": "array",
                    "items": {"type": "object", "properties": {"finding": {"type": "string"}}}
                },
                "methodology": {"type": "object"},
                "significance": {"type": "string"},
                "limitations": {"type": "array", "items": {"type": "string"}},
                "confidence_score": {"type": "number"}
            }
        }

        try:
            # ä½¿ç”¨ç»“æ„åŒ–ç”Ÿæˆ
            analysis = self.llm.generate_structured(
                prompt=prompt,
                output_schema=output_schema,
                system="ä½ æ˜¯èµ„æ·±çš„ç§‘å­¦æ–‡çŒ®åˆ†æå¸ˆã€‚è¯·æä¾›å…¨é¢ã€å‡†ç¡®çš„åˆ†æã€‚",
                max_tokens=2048
            )

            result = PaperAnalysis(
                paper_id=paper.get("id", ""),
                executive_summary=analysis.get("executive_summary", ""),
                key_findings=analysis.get("key_findings", []),
                methodology=analysis.get("methodology", {}),
                significance=analysis.get("significance", ""),
                limitations=analysis.get("limitations", []),
                confidence_score=analysis.get("confidence_score", 0.5),
                analysis_time=time.time() - start_time
            )

            logger.info(f"å®Œæˆè®ºæ–‡æ·±åº¦åˆ†æ: {title}")
            return result

        except Exception as e:
            logger.error(f"è®ºæ–‡æ·±åº¦åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤åˆ†æç»“æœ
            return PaperAnalysis(
                paper_id=paper.get("id", ""),
                executive_summary=f"è®ºæ–‡ã€Š{title}ã€‹çš„åˆ†ææš‚æ—¶ä¸å¯ç”¨ã€‚",
                key_findings=[],
                methodology={},
                significance="",
                limitations=[],
                confidence_score=0.0,
                analysis_time=time.time() - start_time
            )

    def analyze_corpus(self, papers: List[Dict[str, Any]], generate_insights: bool = True) -> Dict[str, Any]:
        """
        åˆ†æè®ºæ–‡è¯­æ–™åº“ï¼Œæå–å…±åŒä¸»é¢˜å’Œè¶‹åŠ¿ã€‚

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            generate_insights: æ˜¯å¦ç”Ÿæˆé«˜å±‚æ´è§

        Returns:
            è¯­æ–™åº“åˆ†æç»“æœ
        """
        analysis = {
            "corpus_size": len(papers),
            "common_themes": [],
            "methodological_trends": {},
            "research_gaps": [],
            "temporal_distribution": {},
            "field_distribution": {}
        }

        if not papers:
            return analysis

        # ç»Ÿè®¡å¹´ä»½åˆ†å¸ƒ
        years = {}
        fields = {}

        for paper in papers:
            year = paper.get("year")
            if year:
                years[year] = years.get(year, 0) + 1

            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„é¢†åŸŸåˆ†æ
            # æš‚æ—¶ç®€åŒ–å¤„ç†

        analysis["temporal_distribution"] = years
        analysis["field_distribution"] = fields

        if generate_insights:
            # ç”Ÿæˆé«˜å±‚æ´è§
            insights_prompt = f"""åˆ†æè¿™ç»„{len(papers)}ç¯‡è®ºæ–‡çš„æ•´ä½“ç‰¹å¾ï¼š

è®ºæ–‡æ ‡é¢˜åˆ—è¡¨ï¼š
{chr(10).join([f"- {p.get('title', '')}" for p in papers[:10]])}

è¯·è¯†åˆ«ï¼š
1. å…±åŒç ”ç©¶ä¸»é¢˜
2. æ–¹æ³•è®ºè¶‹åŠ¿
3. æ½œåœ¨ç ”ç©¶ç©ºç™½

è¯·ç”¨ä¸­æ–‡å›å¤ã€‚"""

            try:
                insights = self.llm.chat([
                    {"role": "system", "content": "ä½ æ˜¯ç§‘ç ”è¶‹åŠ¿åˆ†æå¸ˆ"},
                    {"role": "user", "content": insights_prompt}
                ])

                # ç®€å•è§£æinsightsï¼ˆå¯ä»¥æ”¹è¿›ï¼‰
                analysis["insights_summary"] = insights

            except Exception as e:
                logger.error(f"ç”Ÿæˆæ´è§å¤±è´¥: {e}")
                analysis["insights_summary"] = "æ´è§ç”Ÿæˆå¤±è´¥"

        return analysis
